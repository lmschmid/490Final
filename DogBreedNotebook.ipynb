{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Identification: DLNN Final Project\n",
    "Ian Battin & Liam Schmid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this project, we are tackling the Kaggle Competition: [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification/).\n",
    "\n",
    "In this competition, you are given a dataset containing images of 120 different breeds of dogs. It is our job to classify images in the test set into these 120 different breeds.\n",
    "\n",
    "Being an image based challenge, CNNs are the obvious choice. We will discuss the different strategies and optimizations we tried as well as provide all the code used at each step.\n",
    "\n",
    "## Preparing the data\n",
    "The data is provided in two folders - train and test - where each file is an image. There is a labels.csv file which maps an image filename to a label (breed).\n",
    "\n",
    "To prepare our data for use, we needed to move all of the images in the training set into seperate folders grouped by breed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "    print(\"Usage: python PrepareData.py <labels_file>\")\n",
    "\n",
    "    \n",
    "def generateIDMapping(labelsFile):\n",
    "    idMap = {}\n",
    "\n",
    "    with open(labelsFile) as f:\n",
    "        for line in f:\n",
    "            if line.rstrip() != 0 and line.rstrip() != \"id,breed\":\n",
    "                id, breed = line.rstrip().split(\n",
    "                    ',')[0], line.rstrip().split(',')[1]\n",
    "                idMap[id] = breed\n",
    "    return idMap\n",
    "\n",
    "\n",
    "def makeBreedDirectories(idMap, source, dest):\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "\n",
    "    for filename in os.listdir(source):\n",
    "        id = filename.split('.')[0]\n",
    "        breed = idMap[id]\n",
    "\n",
    "        srcPath = source+\"/\"+filename\n",
    "        destPath = dest+\"/\"+breed+\"/\"+filename\n",
    "\n",
    "        path = Path(dest+\"/\"+breed)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copyfile(srcPath, destPath)\n",
    "\n",
    "\n",
    "def makeValidationDirectories():\n",
    "    for dirname in os.listdir('train'):\n",
    "        samples = np.asarray(os.listdir('train/'+dirname))\n",
    "        valSamples = np.random.choice(samples, len(samples)//4, replace=False)\n",
    "\n",
    "        if os.path.exists('validation/'+dirname):\n",
    "            shutil.rmtree('validation/'+dirname)\n",
    "        for sample in valSamples:\n",
    "            path = Path('validation/'+dirname)\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            shutil.move('train/'+dirname+'/'+sample,\n",
    "                        'validation/'+dirname+'/'+sample)\n",
    "\n",
    "\n",
    "def makeTestDirectory(source, dest):\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "\n",
    "    dest = dest + \"/images\"\n",
    "    path = Path(dest)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(source):\n",
    "        srcPath = source+\"/\"+filename\n",
    "        destPath = dest+\"/\"+filename\n",
    "        shutil.copyfile(srcPath, destPath)\n",
    "\n",
    "\n",
    "idMap = generateIDMapping('data/labels.csv')\n",
    "makeBreedDirectories(idMap, 'data/train', 'train')\n",
    "makeValidationDirectories()\n",
    "makeTestDirectory('data/test', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating image generators\n",
    "Now that our training data is set up, we can begin preparing for training our models.\n",
    "\n",
    "First, we'll need to create image generators for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def get_generators(train_dir_name, val_dir_name, test_dir_name):\n",
    "    # All images will be rescaled by 1./255\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    val_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_data_gen.flow_from_directory(\n",
    "        train_dir_name,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        class_mode=\"categorical\")\n",
    "    val_generator = val_data_gen.flow_from_directory(\n",
    "        val_dir_name,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        class_mode=\"categorical\")\n",
    "    test_generator = test_data_gen.flow_from_directory(\n",
    "        test_dir_name,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(299, 299),\n",
    "        batch_size=1,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    for data_batch, labels_batch in train_generator:\n",
    "        print('data batch shape:', data_batch.shape)\n",
    "        print('labels batch shape:', labels_batch.shape)\n",
    "        break\n",
    "\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "train_dir_name = 'train/'\n",
    "val_dir_name = 'validation/'\n",
    "test_dir_name = 'test/'\n",
    "\n",
    "train_generator, val_generator, test_generator = get_generators(\n",
    "        train_dir_name, val_dir_name, test_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Model\n",
    "We used many different types of models, I'll go in order of our techniques and incremental improvements made\n",
    "\n",
    "### Unmodified InceptionV3\n",
    "Our first test was to simply use the pretrained InceptionV3 model. This model was trained and tested using the ImageNet dataset, of which our dataset is a subset of. This means it should be able to already classify our 120 dog breeds. We simply added a Dense layer of 120 neurons to get an output dimension that matches 120 dog breeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.applications import InceptionV3\n",
    "\n",
    "def build_inception_model():\n",
    "    convBase = InceptionV3(\n",
    "        weights='imagenet', \n",
    "        include_top=True, \n",
    "        input_shape=(299, 299, 3))\n",
    "    convBase.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(convBase)\n",
    "    model.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_inception_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train it and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_gen, val_gen, epochs=30, verbose=False):\n",
    "    history = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=50,\n",
    "        verbose=verbose)\n",
    "\n",
    "    return history\n",
    "\n",
    "train_model(model, train_generator, val_generator, \n",
    "    epochs=50, verbose=True)\n",
    "model.save(\"./Inception.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data, we can already see that it's doing decent, but not great. It's only reaching accuracies in the mid 70s, and validation accuracys in the mid 80s. Lets go ahead and generate predictions for our test set and submit to kaggle to see our score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(model, label_map, test_generator, verbose=False):\n",
    "    steps = test_generator.n\n",
    "\n",
    "    predictions = model.predict_generator(\n",
    "        test_generator,\n",
    "        steps=steps,\n",
    "        verbose=verbose)\n",
    "\n",
    "    labels = sorted(list(label_map.keys()))\n",
    "    \n",
    "    with open(\"predictions.csv\", 'w') as pred_file:\n",
    "        pred_file.write('id,{}\\n'.format(\",\".join(labels)))\n",
    "        for index, prediction in enumerate(predictions):\n",
    "            id = re.split(\"[./]\", test_generator.filenames[index])[-2]\n",
    "            prediction_list = prediction.tolist()\n",
    "            confidence_vals = \",\".join(map(str, prediction_list))\n",
    "            pred_file.write(\"{},{}\\n\".format(id, confidence_vals))\n",
    "\n",
    "            if verbose:\n",
    "                max_val = labels[prediction_list.index(max(prediction_list))]\n",
    "                print(\"Image '{}' classified as a {}\".format(id, max_val))\n",
    "                \n",
    "classify_images(\n",
    "    model, \n",
    "    train_generator.class_indices, \n",
    "    test_generator, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting the outputted 'predictions.csv' file on kaggle gives us a log loss score of 2.86574 which isn't great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model using extacted features from InceptionV3\n",
    "The default inception model is trained to classify 1000 labels, of which our 120 is a subset of. That's a lot of overkill and we could benefit by simply extracting the features from it and adding our own custom dense layer ontop of that. We also added some Dropout layers to prevent overfitting as well as changing the optimzer which seemed to improve the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_inception_model():\n",
    "    convBase = InceptionV3(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=(299, 299, 3))\n",
    "    convBase.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(convBase)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(768, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "    optimizer = optimizers.SGD(lr=0.001, momentum=0.09)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_custom_inception_model()\n",
    "train_model(model, train_generator, val_generator, \n",
    "    epochs=50, verbose=True)\n",
    "model.save(\"./Inception.h5\")\n",
    "\n",
    "classify_images(\n",
    "    model, \n",
    "    train_generator.class_indices, \n",
    "    test_generator, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our accuracy has jumped to around 97-98% by 50 epochs, with a valiation accuracy around the same. This is a huge improvement. Submitting this to kaggle gets us a score of 0.46835."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
